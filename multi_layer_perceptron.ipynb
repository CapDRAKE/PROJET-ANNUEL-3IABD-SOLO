{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Neuron():\n",
    "    '''\n",
    "        A conceptual Neuron hat can be trained using a \n",
    "        fit and predict methodology, without any library\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, position_in_layer, is_output_neuron=False, is_sigmoid=True):\n",
    "        self.weights = []\n",
    "        self.inputs = []\n",
    "        self.output = None\n",
    "        \n",
    "        # This is used for the backpropagation update\n",
    "        self.updated_weights = []\n",
    "        # This is used to know how to update the weights\n",
    "        self.is_output_neuron = is_output_neuron\n",
    "        # This delta is used for the update at the backpropagation\n",
    "        self.delta = None\n",
    "        # This is used for the backpropagation update\n",
    "        self.position_in_layer = position_in_layer \n",
    "        self.is_sigmoid = is_sigmoid\n",
    "        \n",
    "    def attach_to_output(self, neurons):\n",
    "        '''\n",
    "            Helper function to store the reference of the other neurons\n",
    "            To this particular neuron (used for backpropagation)\n",
    "        '''\n",
    "        self.output_neurons = neurons\n",
    "    \n",
    "    def activation(self, x):\n",
    "        if self.is_sigmoid:\n",
    "            return self.sigmoid(x)\n",
    "        else:\n",
    "            return self.Relu(x)\n",
    "\n",
    "    def activationDerive(self, x):\n",
    "        if self.is_sigmoid:\n",
    "            return self.sigmoidDerive(x)\n",
    "        else:\n",
    "            return self.ReluDerive(x)\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        '''\n",
    "            simple sigmoid function (logistic) used for the activation\n",
    "        '''\n",
    "        if isinstance(x, list):\n",
    "            print(x)\n",
    "        return 1 / (1 + math.exp(-x))\n",
    "\n",
    "    def sigmoidDerive(self, x):\n",
    "        '''\n",
    "            simple sigmoid function (logistic) used for the activation\n",
    "        '''\n",
    "        return x*(1-x)\n",
    "\n",
    "    def Relu(self, x):\n",
    "        '''\n",
    "            simple Relu function (logistic) used for the activation\n",
    "        '''\n",
    "        return x if x > 0 else 0\n",
    "\n",
    "    def ReluDerive(self, x):\n",
    "        '''\n",
    "            simple ReluDerive function (logistic) used for the activation\n",
    "        '''\n",
    "        return 1 if x > 0 else 0\n",
    "    \n",
    "    def init_weights(self, num_input):\n",
    "        '''\n",
    "            This is used to setup the weights when we know how many inputs there is for\n",
    "            a given neuron\n",
    "        '''\n",
    "        # Randomly initalize the weights\n",
    "        for i in range(num_input+1):\n",
    "            w=random.uniform(0,1)\n",
    "            self.weights.append(w)\n",
    "            self.updated_weights.append(w)\n",
    "        \n",
    "    def predict(self, row):\n",
    "        '''\n",
    "            Given a row of data it will predict what the output should be for\n",
    "            this given neuron. We can have many input, but only one output for a neuron\n",
    "        '''\n",
    "        \n",
    "        # Reset the inputs\n",
    "        self.inputs = []\n",
    "        \n",
    "        # We iterate over the weights and the features in the given row\n",
    "        activation = 0\n",
    "        for weight, feature in zip(self.weights, row):\n",
    "            self.inputs.append(feature)\n",
    "            activation = activation + weight*feature\n",
    "            \n",
    "        \n",
    "        self.output = self.activation(activation)\n",
    "        return self.output\n",
    "    \n",
    "        \n",
    "            \n",
    "    def update_neuron(self):\n",
    "        '''\n",
    "            Will update a given neuron weights by replacing the current weights\n",
    "            with those used during the backpropagation. This need to be done at the end of the\n",
    "            backpropagation\n",
    "        '''\n",
    "        \n",
    "        self.weights = []\n",
    "        for new_weight in self.updated_weights:\n",
    "            self.weights.append(new_weight)\n",
    "    \n",
    "    def calculate_update(self, learning_rate, target, batchSize):\n",
    "        '''\n",
    "            This function will calculate the updated weights for this neuron. It will first calculate\n",
    "            the right delta (depending if this neuron is a ouput or a hidden neuron), then it will\n",
    "            calculate the right updated_weights. It will not overwrite the weights yet as they are needed\n",
    "            for other update in the backpropagation algorithm.\n",
    "        '''\n",
    "        derive = self.activationDerive(self.output)\n",
    "        if self.is_output_neuron:\n",
    "            # Calculate the delta for the output\n",
    "            self.delta = (self.output - target[self.position_in_layer])*derive\n",
    "        else:\n",
    "            # Calculate the delta\n",
    "            delta_sum = 0\n",
    "            # this is to know which weights this neuron is contributing in the output layer\n",
    "            cur_weight_index = self.position_in_layer \n",
    "            for output_neuron in self.output_neurons:\n",
    "                delta_sum = delta_sum + (output_neuron.delta * output_neuron.weights[cur_weight_index])\n",
    "\n",
    "            # Update this neuron delta\n",
    "            self.delta = delta_sum*derive\n",
    "        \n",
    "        # Iterate over each weight and update them\n",
    "        i=0\n",
    "        for cur_weight, cur_input in zip(self.weights, self.inputs):\n",
    "            self.updated_weights[i]-=(learning_rate*self.delta*cur_input)/batchSize\n",
    "            i+=1\n",
    "\n",
    "    def toString(self):\n",
    "        print(self.weights)\n",
    "         \n",
    "class Layer():\n",
    "    '''\n",
    "        Layer is modelizing a layer in the fully-connected-feedforward neural network architecture.\n",
    "        It will play the role of connecting everything together inside and will be doing the backpropagation \n",
    "        update.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, num_neuron, is_output_layer = False, is_sigmoid=True):\n",
    "        \n",
    "        # Will create that much neurons in this layer\n",
    "        self.is_output_layer = is_output_layer\n",
    "        self.neurons = []\n",
    "        for i in range(num_neuron):\n",
    "            # Create neuron\n",
    "            neuron = Neuron(i,  is_output_neuron=is_output_layer, is_sigmoid=is_sigmoid)\n",
    "            self.neurons.append(neuron)\n",
    "    \n",
    "    def attach(self, layer):\n",
    "        '''\n",
    "            This function attach the neurons from this layer to another one\n",
    "            This is needed for the backpropagation algorithm\n",
    "        '''\n",
    "        # Iterate over the neurons in the current layer and attach \n",
    "        # them to the next layer\n",
    "        for in_neuron in self.neurons:\n",
    "            in_neuron.attach_to_output(layer.neurons)\n",
    "            \n",
    "    def init_layer(self, num_input):\n",
    "        '''\n",
    "            This will initialize the weights of each neuron in the layer.\n",
    "            By giving the right num_input it will spawn the right number of weights\n",
    "        '''\n",
    "        \n",
    "        # Iterate over each of the neuron and initialize\n",
    "        # the weights that connect with the previous layer\n",
    "        for neuron in self.neurons:\n",
    "            neuron.init_weights(num_input)\n",
    "    \n",
    "    def predict(self, row):\n",
    "        '''\n",
    "            This will calcualte the activations for the full layer given the row of data \n",
    "            streaming in.\n",
    "        '''\n",
    "        rowClone = row.copy()\n",
    "        rowClone = np.append(rowClone, [1]) # need to add the bias\n",
    "        activations = [neuron.predict(rowClone) for neuron in self.neurons]\n",
    "        return activations\n",
    "\n",
    "    def toString(self):\n",
    "        for neuron in self.neurons:\n",
    "            neuron.toString(); \n",
    "        \n",
    "class MultiLayerPerceptron():\n",
    "    '''\n",
    "        We will be creating the multi-layer perceptron with only two layer:\n",
    "        an input layer, a perceptrons layer and a one neuron output layer which does binary classification\n",
    "    '''\n",
    "    def __init__(self, is_Classifier=True):\n",
    "        \n",
    "        random.seed(5)\n",
    "        # Layers\n",
    "        self.layers = []\n",
    "                \n",
    "        # Training parameters\n",
    "        self.is_Classifier = is_Classifier\n",
    "        \n",
    "        \n",
    "    def add_output_layer(self, num_neuron, is_sigmoid=True):\n",
    "        '''\n",
    "            This helper function will create a new output layer and add it to the architecture\n",
    "        '''\n",
    "        self.layers.insert(0, Layer(num_neuron, is_output_layer = True, is_sigmoid=is_sigmoid))\n",
    "    \n",
    "    def add_hidden_layer(self, num_neuron, is_sigmoid=True):\n",
    "        '''\n",
    "            This helper function will create a new hidden layer, add it to the architecture\n",
    "            and finally attach it to the front of the architecture\n",
    "        '''\n",
    "        # Create an hidden layer\n",
    "        hidden_layer = Layer(num_neuron, is_sigmoid=is_sigmoid)\n",
    "        # Attach the last added layer to this new layer\n",
    "        hidden_layer.attach(self.layers[0])\n",
    "        # Add this layers to the architecture\n",
    "        self.layers.insert(0, hidden_layer)\n",
    "\n",
    "    def learn_layers(self, target,learning_rate,batchSize):\n",
    "        '''\n",
    "            Will update all the layers by calculating the updated weights and then updating \n",
    "            the weights all at once when the new weights are found.\n",
    "        '''\n",
    "        # Iterate over each of the layer in reverse order\n",
    "        # to calculate the updated weights\n",
    "        for layer in reversed(self.layers):\n",
    "            # Calculate update the hidden layer\n",
    "            for neuron in layer.neurons:\n",
    "                neuron.calculate_update(learning_rate, target,batchSize)\n",
    "        \n",
    "    def update_layers(self):\n",
    "        '''\n",
    "            Will update all the layers by calculating the updated weights and then updating \n",
    "            the weights all at once when the new weights are found.\n",
    "        '''        \n",
    "        # Iterate over each of the layer in normal order\n",
    "        # to update the weights\n",
    "        for layer in self.layers:\n",
    "            for neuron in layer.neurons:\n",
    "                neuron.update_neuron()\n",
    "    \n",
    "    def train(self, X, y,batchSize=100,epochs=40,learning_rate = 0.1):\n",
    "        '''\n",
    "            Main training function of the neural network algorithm. This will make use of backpropagation.\n",
    "            It will use stochastic gradient descent by selecting one row at random from the dataset and \n",
    "            use predict to calculate the error. The error will then be backpropagated and new weights calculated.\n",
    "            Once all the new weights are calculated, the whole network weights will be updated\n",
    "        '''\n",
    "\n",
    "        \n",
    "        num_row = len(X)\n",
    "        num_feature = len(X[0]) # Here we assume that we have a rectangular matrix\n",
    "        \n",
    "        # Init the weights throughout each of the layer\n",
    "        self.layers[0].init_layer(num_feature)\n",
    "        \n",
    "        for i in range(1, len(self.layers)):\n",
    "            num_input = len(self.layers[i-1].neurons)\n",
    "            self.layers[i].init_layer(num_input)\n",
    "        \n",
    "        #self.toString()\n",
    "\n",
    "        # Launch the training algorithm\n",
    "        for i in range(batchSize*epochs):\n",
    "            \n",
    "            # Stochastic Gradient Descent\n",
    "            r_i = random.randint(0,num_row-1)\n",
    "            row = X[r_i] # take the random sample from the dataset\n",
    "            yhat = self.predict([row])[0]\n",
    "            target = y[r_i]\n",
    "            \n",
    "            # Update the layers using backpropagation   \n",
    "            self.learn_layers( target,learning_rate,batchSize)\n",
    "            if (i+1) % batchSize == 0:\n",
    "                self.update_layers()\n",
    "            \n",
    "            # At every 100 iteration we calculate the error\n",
    "            # on the whole training set\n",
    "            if i % 100 == 0:\n",
    "                total_error = 0\n",
    "\n",
    "                for r_i in range(num_row):\n",
    "                    row = X[r_i]\n",
    "                    yhat = self.predict([row])[0]\n",
    "                    error = (y[r_i] - yhat)\n",
    "                    total_error = total_error + error**2\n",
    "                mean_error = total_error/num_row\n",
    "                print(f\"Iteration {i} with error = {mean_error}\")\n",
    "                \n",
    "        #self.toString()\n",
    "        \n",
    "    \n",
    "    def predict(self, rows):\n",
    "        '''\n",
    "            Prediction function that will take a row of input and give back the output\n",
    "            of the whole neural network.\n",
    "        '''\n",
    "        \n",
    "        # Gather all the activation in the hidden layer\n",
    "        results =[]\n",
    "        # Gather all the activation in the hidden layer\n",
    "        for row in rows:\n",
    "            activations = self.layers[0].predict(row)\n",
    "            for i in range(1, len(self.layers)):\n",
    "                activations = self.layers[i].predict(activations)\n",
    "\n",
    "            outputs = []\n",
    "\n",
    "            for activation in activations:\n",
    "                if self.is_Classifier :\n",
    "                    # Decide if we output a 1 or 0\n",
    "                    if activation >= 0.5:\n",
    "                        outputs.append(1.0)\n",
    "                    else:\n",
    "                        outputs.append(0.0)\n",
    "                else:\n",
    "                    outputs.append(activation)\n",
    "            results.append(outputs)\n",
    "        return results\n",
    "\n",
    "    def validate(self, rows,rowY):\n",
    "        '''\n",
    "            Prediction function that will take a row of input and give back the output\n",
    "            of the whole neural network.\n",
    "        '''\n",
    "        total_error = 0\n",
    "        num_row= len(rows)\n",
    "        for r_i in range(num_row):\n",
    "            row = rows[r_i]\n",
    "            yhat = self.predict([row])[0]\n",
    "            print( \"prediction \",  yhat,\"  reelle\",rowY[r_i])\n",
    "            error = (rowY[r_i] - yhat)\n",
    "            total_error = total_error + error**2\n",
    "        mean_error = total_error/num_row\n",
    "        print(f\"Validation error = {mean_error}\")\n",
    "\n",
    "\n",
    "    def toString(self):\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            print(\"Layer\"+str(i))\n",
    "            layer.toString()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 with error = [0.5]\n",
      "Iteration 100 with error = [0.5]\n",
      "Iteration 200 with error = [0.5]\n",
      "Iteration 300 with error = [0.5]\n",
      "Iteration 400 with error = [0.75]\n",
      "Iteration 500 with error = [0.5]\n",
      "Iteration 600 with error = [0.5]\n",
      "Iteration 700 with error = [0.5]\n",
      "Iteration 800 with error = [0.5]\n",
      "Iteration 900 with error = [0.5]\n",
      "Iteration 1000 with error = [0.5]\n",
      "Iteration 1100 with error = [0.5]\n",
      "Iteration 1200 with error = [0.5]\n",
      "Iteration 1300 with error = [0.5]\n",
      "Iteration 1400 with error = [0.5]\n",
      "Iteration 1500 with error = [0.5]\n",
      "Iteration 1600 with error = [0.5]\n",
      "Iteration 1700 with error = [0.5]\n",
      "Iteration 1800 with error = [0.5]\n",
      "Iteration 1900 with error = [0.5]\n",
      "Iteration 2000 with error = [0.5]\n",
      "Iteration 2100 with error = [0.5]\n",
      "Iteration 2200 with error = [0.5]\n",
      "Iteration 2300 with error = [0.5]\n",
      "Iteration 2400 with error = [0.5]\n",
      "Iteration 2500 with error = [0.5]\n",
      "Iteration 2600 with error = [0.5]\n",
      "Iteration 2700 with error = [0.5]\n",
      "Iteration 2800 with error = [0.5]\n",
      "Iteration 2900 with error = [0.5]\n",
      "Iteration 3000 with error = [0.5]\n",
      "Iteration 3100 with error = [0.5]\n",
      "Iteration 3200 with error = [0.5]\n",
      "Iteration 3300 with error = [0.5]\n",
      "Iteration 3400 with error = [0.5]\n",
      "Iteration 3500 with error = [0.5]\n",
      "Iteration 3600 with error = [0.5]\n",
      "Iteration 3700 with error = [0.5]\n",
      "Iteration 3800 with error = [0.5]\n",
      "Iteration 3900 with error = [0.5]\n",
      "Iteration 4000 with error = [0.5]\n",
      "Iteration 4100 with error = [0.5]\n",
      "Iteration 4200 with error = [0.5]\n",
      "Iteration 4300 with error = [0.5]\n",
      "Iteration 4400 with error = [0.5]\n",
      "Iteration 4500 with error = [0.5]\n",
      "Iteration 4600 with error = [0.5]\n",
      "Iteration 4700 with error = [0.5]\n",
      "Iteration 4800 with error = [0.5]\n",
      "Iteration 4900 with error = [0.5]\n",
      "Iteration 5000 with error = [0.5]\n",
      "Iteration 5100 with error = [0.5]\n",
      "Iteration 5200 with error = [0.5]\n",
      "Iteration 5300 with error = [0.5]\n",
      "Iteration 5400 with error = [0.75]\n",
      "Iteration 5500 with error = [0.5]\n",
      "Iteration 5600 with error = [0.25]\n",
      "Iteration 5700 with error = [0.5]\n",
      "Iteration 5800 with error = [0.5]\n",
      "Iteration 5900 with error = [0.5]\n",
      "Iteration 6000 with error = [0.5]\n",
      "Iteration 6100 with error = [0.5]\n",
      "Iteration 6200 with error = [0.5]\n",
      "Iteration 6300 with error = [0.5]\n",
      "Iteration 6400 with error = [0.25]\n",
      "Iteration 6500 with error = [0.25]\n",
      "Iteration 6600 with error = [0.25]\n",
      "Iteration 6700 with error = [0.25]\n",
      "Iteration 6800 with error = [0.5]\n",
      "Iteration 6900 with error = [0.25]\n",
      "Iteration 7000 with error = [0.5]\n",
      "Iteration 7100 with error = [0.25]\n",
      "Iteration 7200 with error = [0.25]\n",
      "Iteration 7300 with error = [0.25]\n",
      "Iteration 7400 with error = [0.25]\n",
      "Iteration 7500 with error = [0.25]\n",
      "Iteration 7600 with error = [0.25]\n",
      "Iteration 7700 with error = [0.5]\n",
      "Iteration 7800 with error = [0.25]\n",
      "Iteration 7900 with error = [0.25]\n",
      "Iteration 8000 with error = [0.25]\n",
      "Iteration 8100 with error = [0.75]\n",
      "Iteration 8200 with error = [0.25]\n",
      "Iteration 8300 with error = [0.25]\n",
      "Iteration 8400 with error = [0.25]\n",
      "Iteration 8500 with error = [0.25]\n",
      "Iteration 8600 with error = [0.25]\n",
      "Iteration 8700 with error = [0.25]\n",
      "Iteration 8800 with error = [0.25]\n",
      "Iteration 8900 with error = [0.25]\n",
      "Iteration 9000 with error = [0.25]\n",
      "Iteration 9100 with error = [0.25]\n",
      "Iteration 9200 with error = [0.5]\n",
      "Iteration 9300 with error = [0.25]\n",
      "Iteration 9400 with error = [0.25]\n",
      "Iteration 9500 with error = [0.25]\n",
      "Iteration 9600 with error = [0.25]\n",
      "Iteration 9700 with error = [0.25]\n",
      "Iteration 9800 with error = [0.25]\n",
      "Iteration 9900 with error = [0.25]\n",
      "Iteration 10000 with error = [0.25]\n",
      "Iteration 10100 with error = [0.25]\n",
      "Iteration 10200 with error = [0.25]\n",
      "Iteration 10300 with error = [0.25]\n",
      "Iteration 10400 with error = [0.25]\n",
      "Iteration 10500 with error = [0.25]\n",
      "Iteration 10600 with error = [0.25]\n",
      "Iteration 10700 with error = [0.25]\n",
      "Iteration 10800 with error = [0.25]\n",
      "Iteration 10900 with error = [0.25]\n",
      "Iteration 11000 with error = [0.25]\n",
      "Iteration 11100 with error = [0.25]\n",
      "Iteration 11200 with error = [0.25]\n",
      "Iteration 11300 with error = [0.25]\n",
      "Iteration 11400 with error = [0.25]\n",
      "Iteration 11500 with error = [0.25]\n",
      "Iteration 11600 with error = [0.25]\n",
      "Iteration 11700 with error = [0.5]\n",
      "Iteration 11800 with error = [0.25]\n",
      "Iteration 11900 with error = [0.25]\n",
      "Iteration 12000 with error = [0.25]\n",
      "Iteration 12100 with error = [0.25]\n",
      "Iteration 12200 with error = [0.25]\n",
      "Iteration 12300 with error = [0.25]\n",
      "Iteration 12400 with error = [0.25]\n",
      "Iteration 12500 with error = [0.25]\n",
      "Iteration 12600 with error = [0.25]\n",
      "Iteration 12700 with error = [0.25]\n",
      "Iteration 12800 with error = [0.25]\n",
      "Iteration 12900 with error = [0.25]\n",
      "Iteration 13000 with error = [0.25]\n",
      "Iteration 13100 with error = [0.5]\n",
      "Iteration 13200 with error = [0.25]\n",
      "Iteration 13300 with error = [0.25]\n",
      "Iteration 13400 with error = [0.25]\n",
      "Iteration 13500 with error = [0.25]\n",
      "Iteration 13600 with error = [0.25]\n",
      "Iteration 13700 with error = [0.25]\n",
      "Iteration 13800 with error = [0.25]\n",
      "Iteration 13900 with error = [0.25]\n",
      "Iteration 14000 with error = [0.25]\n",
      "Iteration 14100 with error = [0.25]\n",
      "Iteration 14200 with error = [0.25]\n",
      "Iteration 14300 with error = [0.25]\n",
      "Iteration 14400 with error = [0.25]\n",
      "Iteration 14500 with error = [0.25]\n",
      "Iteration 14600 with error = [0.25]\n",
      "Iteration 14700 with error = [0.25]\n",
      "Iteration 14800 with error = [0.25]\n",
      "Iteration 14900 with error = [0.25]\n",
      "Iteration 15000 with error = [0.25]\n",
      "Iteration 15100 with error = [0.25]\n",
      "Iteration 15200 with error = [0.25]\n",
      "Iteration 15300 with error = [0.25]\n",
      "Iteration 15400 with error = [0.25]\n",
      "Iteration 15500 with error = [0.25]\n",
      "Iteration 15600 with error = [0.25]\n",
      "Iteration 15700 with error = [0.25]\n",
      "Iteration 15800 with error = [0.25]\n",
      "Iteration 15900 with error = [0.25]\n",
      "Iteration 16000 with error = [0.25]\n",
      "Iteration 16100 with error = [0.25]\n",
      "Iteration 16200 with error = [0.25]\n",
      "Iteration 16300 with error = [0.25]\n",
      "Iteration 16400 with error = [0.]\n",
      "Iteration 16500 with error = [0.25]\n",
      "Iteration 16600 with error = [0.25]\n",
      "Iteration 16700 with error = [0.25]\n",
      "Iteration 16800 with error = [0.25]\n",
      "Iteration 16900 with error = [0.]\n",
      "Iteration 17000 with error = [0.]\n",
      "Iteration 17100 with error = [0.]\n",
      "Iteration 17200 with error = [0.]\n",
      "Iteration 17300 with error = [0.]\n",
      "Iteration 17400 with error = [0.]\n",
      "Iteration 17500 with error = [0.]\n",
      "Iteration 17600 with error = [0.]\n",
      "Iteration 17700 with error = [0.]\n",
      "Iteration 17800 with error = [0.]\n",
      "Iteration 17900 with error = [0.]\n",
      "Iteration 18000 with error = [0.]\n",
      "Iteration 18100 with error = [0.]\n",
      "Iteration 18200 with error = [0.]\n",
      "Iteration 18300 with error = [0.]\n",
      "Iteration 18400 with error = [0.]\n",
      "Iteration 18500 with error = [0.]\n",
      "Iteration 18600 with error = [0.]\n",
      "Iteration 18700 with error = [0.]\n",
      "Iteration 18800 with error = [0.]\n",
      "Iteration 18900 with error = [0.]\n",
      "Iteration 19000 with error = [0.]\n",
      "Iteration 19100 with error = [0.]\n",
      "Iteration 19200 with error = [0.]\n",
      "Iteration 19300 with error = [0.]\n",
      "Iteration 19400 with error = [0.]\n",
      "Iteration 19500 with error = [0.]\n",
      "Iteration 19600 with error = [0.]\n",
      "Iteration 19700 with error = [0.]\n",
      "Iteration 19800 with error = [0.]\n",
      "Iteration 19900 with error = [0.]\n"
     ]
    }
   ],
   "source": [
    "# XOR function (one or the other but not both)\n",
    "X = np.array([[0,0], [0,1], [1,0], [1,1]])\n",
    "y = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "# Init the parameters for the network\n",
    "clf = MultiLayerPerceptron( )\n",
    "# Create the architecture backward\n",
    "clf.add_output_layer(num_neuron = 1)\n",
    "#clf.add_hidden_layer(num_neuron = 3)\n",
    "clf.add_hidden_layer(num_neuron = 2)\n",
    "# Train the network\n",
    "clf.train(X,y,learning_rate = 1,batchSize=10,epochs=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected 0.0, got:  [[0.0]]\n",
      "Expected 1.0, got:  [[1.0]]\n",
      "Expected 1.0, got:  [[1.0]]\n",
      "Expected 0.0, got:  [[0.0]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Expected 0.0, got: \",clf.predict([[0,0]]))\n",
    "print(\"Expected 1.0, got: \",clf.predict([[0,1]]))\n",
    "print(\"Expected 1.0, got: \",clf.predict([[1,0]]))\n",
    "print(\"Expected 0.0, got: \",clf.predict([[1,1]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer0\n",
      "[0.6229016948897019, 0.7417869892607294]\n",
      "[0.7951935655656966, 0.9424502837770503]\n",
      "[0.7398985747399307, 0.922324996665417]\n",
      "[0.029005228283614737, 0.46562265437810535]\n",
      "[0.9433567169983137, 0.6489745531369242]\n",
      "Layer1\n",
      "[0.9009004917506227, 0.11320596465314436, 0.46906904778216374, 0.24657283261983032, 0.5437608592359304, 0.5739411879281008]\n",
      "Iteration 0 with error = 0.12512867255686413\n",
      "Iteration 10000 with error = 0.0508024048067219\n",
      "Iteration 20000 with error = 0.0005740217020324812\n",
      "Iteration 30000 with error = 7.019788673630293e-08\n",
      "Iteration 40000 with error = 5.616885089795582e-12\n",
      "Iteration 50000 with error = 6.311150612722709e-16\n",
      "Iteration 60000 with error = 7.215746722103003e-20\n",
      "Iteration 70000 with error = 8.416529298172356e-24\n",
      "Iteration 80000 with error = 7.192439303352575e-28\n",
      "Iteration 90000 with error = 1.0518145402946823e-30\n",
      "Layer0\n",
      "[2.8245779778904674, -2.3878341052405223]\n",
      "[0.47721395054360394, 0.2380056219798162]\n",
      "[1.7017119033945256, 0.42518754329611186]\n",
      "[0.9136638742670695, -1.7975015119193758]\n",
      "[2.498921596860624, -1.532350230023126]\n",
      "Layer1\n",
      "[3.070287103888726, -1.1591096164060208, 0.6508517641503079, -2.804933151836004, 2.311273736971946, -0.5225262324014006]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# 1. Creation of test data set\n",
    "\n",
    "X = np.array([\n",
    "      [1],\n",
    "      [2],\n",
    "      [3]\n",
    "])\n",
    "Y = np.array([\n",
    "      2,\n",
    "      3,\n",
    "      2.5\n",
    "])\n",
    "\n",
    "\n",
    "# Init the parameters for the network\n",
    "clf = MultiLayerPerceptron(learning_rate = 0.05, num_iteration = 100000, is_Classifier = False)\n",
    "# Create the architecture backward\n",
    "clf.add_output_layer(num_neuron = 1, is_sigmoid=False)\n",
    "clf.add_hidden_layer(num_neuron = 5)\n",
    "# Train the network\n",
    "clf.train(X,Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected 2, got:  1.9999999999999998\n",
      "Expected 3, got:  2.999999999999999\n",
      "Expected 2.5, got:  2.500000000000001\n"
     ]
    }
   ],
   "source": [
    "print(\"Expected 2, got: \",clf.predict([1]))\n",
    "print(\"Expected 3, got: \",clf.predict([2]))\n",
    "print(\"Expected 2.5, got: \",clf.predict([3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer0\n",
      "[0.6229016948897019, 0.7417869892607294, 0.7951935655656966]\n",
      "[0.9424502837770503, 0.7398985747399307, 0.922324996665417]\n",
      "[0.029005228283614737, 0.46562265437810535, 0.9433567169983137]\n",
      "[0.6489745531369242, 0.9009004917506227, 0.11320596465314436]\n",
      "[0.46906904778216374, 0.24657283261983032, 0.5437608592359304]\n",
      "[0.5739411879281008, 0.013114189588902203, 0.21672980046384815]\n",
      "[0.2794823660111103, 0.9163453718085519, 0.7657254516291417]\n",
      "[0.15960421235803823, 0.7971469914312045, 0.13876741839890316]\n",
      "[0.6174525204661166, 0.1266992325502697, 0.0017748622025346439]\n",
      "[0.8714047447242821, 0.2094563824951179, 0.21548116922473226]\n",
      "Layer1\n",
      "[0.9824211088259253, 0.8724077654368019, 0.2893051677469265, 0.9614779889500835, 0.5392234688708106, 0.6778304772505923, 0.20477951453379284, 0.9409760010879991, 0.6906419411069082, 0.9665643123171954, 0.8937416775764785]\n",
      "Iteration 0 with error = 0.4957854564830943\n",
      "Iteration 10000 with error = 2.3458856350463867e-25\n",
      "Iteration 20000 with error = 0.0\n",
      "Iteration 30000 with error = 0.0\n",
      "Iteration 40000 with error = 0.0\n",
      "Iteration 50000 with error = 0.0\n",
      "Iteration 60000 with error = 0.0\n",
      "Iteration 70000 with error = 0.0\n",
      "Iteration 80000 with error = 0.0\n",
      "Iteration 90000 with error = 0.0\n",
      "Layer0\n",
      "[0.35829775689073695, 0.5937278924749089, 0.42432673665613474]\n",
      "[0.8348652550465763, 0.6502806686366484, 0.8041626822256701]\n",
      "[-0.20552335257155505, 0.1270731809701314, 0.9634405427455407]\n",
      "[0.2451407746877986, 0.842142206294389, -0.714123312262851]\n",
      "[0.362431017668257, 0.21293285217536195, 0.41669930367545294]\n",
      "[0.5655926020724974, 0.17386605389391976, -0.024537565690777193]\n",
      "[0.4156623890650826, 0.9934864040810493, 0.9490410998200118]\n",
      "[-0.19466943962115996, 1.2809720563091953, -0.8324658634743921]\n",
      "[0.6103499202163367, 0.3459093189035704, -0.39139159587774186]\n",
      "[0.7604428729705127, 0.28149391188501854, -0.4015162673495689]\n",
      "Layer1\n",
      "[0.45668229416739536, 0.0076672053388575035, -0.42717560410440114, 1.1956851044431287, 0.08859622088394115, 0.36904424963447147, -0.4845395097304965, 1.521941626762409, 0.6942611194522414, 0.919651166097551, -0.5489984522398309]\n"
     ]
    }
   ],
   "source": [
    "# 1. Creation of test data set\n",
    "\n",
    "X = np.array([\n",
    "      [1, 1],\n",
    "      [2, 2],\n",
    "      [3, 1]\n",
    "])\n",
    "Y = np.array([\n",
    "      2,\n",
    "      3,\n",
    "      2.5\n",
    "])\n",
    "\n",
    "# Init the parameters for the network\n",
    "clf = MultiLayerPerceptron(learning_rate = 0.1, num_iteration = 100000, is_Classifier = False)\n",
    "# Create the architecture backward\n",
    "clf.add_output_layer(num_neuron = 1, is_sigmoid=False)\n",
    "clf.add_hidden_layer(num_neuron = 10)\n",
    "# Train the network\n",
    "clf.train(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected 2, got:  2.0\n",
      "Expected 3, got:  3.0\n",
      "Expected 2.5, got:  2.5\n"
     ]
    }
   ],
   "source": [
    "print(\"Expected 2, got: \",clf.predict([1, 1]))\n",
    "print(\"Expected 3, got: \",clf.predict([2, 2]))\n",
    "print(\"Expected 2.5, got: \",clf.predict([3, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer0\n",
      "[0.6229016948897019, 0.7417869892607294, 0.7951935655656966]\n",
      "[0.9424502837770503, 0.7398985747399307, 0.922324996665417]\n",
      "[0.029005228283614737, 0.46562265437810535, 0.9433567169983137]\n",
      "[0.6489745531369242, 0.9009004917506227, 0.11320596465314436]\n",
      "[0.46906904778216374, 0.24657283261983032, 0.5437608592359304]\n",
      "Layer1\n",
      "[0.5739411879281008, 0.013114189588902203, 0.21672980046384815, 0.2794823660111103, 0.9163453718085519, 0.7657254516291417]\n",
      "[0.15960421235803823, 0.7971469914312045, 0.13876741839890316, 0.6174525204661166, 0.1266992325502697, 0.0017748622025346439]\n",
      "Layer2\n",
      "[0.8714047447242821, 0.2094563824951179, 0.21548116922473226]\n",
      "Iteration 0 with error = 3.7764242300194795\n",
      "Iteration 10000 with error = 2.317736223709652e-22\n",
      "Iteration 20000 with error = 1.639351568662415e-30\n",
      "Iteration 30000 with error = 8.38164711797325e-31\n",
      "Iteration 40000 with error = 5.669937756276022e-31\n",
      "Iteration 50000 with error = 1.7256332301709633e-31\n",
      "Iteration 60000 with error = 1.9721522630525295e-31\n",
      "Iteration 70000 with error = 1.1093356479670479e-31\n",
      "Iteration 80000 with error = 1.232595164407831e-32\n",
      "Iteration 90000 with error = 1.232595164407831e-32\n",
      "Layer0\n",
      "[0.4088006408881912, 0.6610932543684893, 0.451600106903821]\n",
      "[-1.667300284223685, 0.9989378115419957, 0.05416277667934154]\n",
      "[-3.3636286822630184, 4.313587826887995, 1.4270243300568437]\n",
      "[-2.6291928172792964, 2.3934636902984567, -0.5476838216957055]\n",
      "[3.086402035160415, -1.3668337580205863, -0.08305899816914286]\n",
      "Layer1\n",
      "[-0.010110392947831823, -0.9021913413201837, -5.484178372116089, -3.1879537893975036, 2.013132270131939, 0.9814573966294403]\n",
      "[-0.3345520153946585, 1.7099700878613164, -0.10598481725355173, 2.235625891467345, -3.34159773901057, -0.4469488411593395]\n",
      "Layer2\n",
      "[5.011027874419526, 4.089677440459327, -2.4915851240161278]\n"
     ]
    }
   ],
   "source": [
    "# 1. Creation of test data set\n",
    "\n",
    "X = np.array([\n",
    "      [1, 0],\n",
    "      [0, 1],\n",
    "      [1, 1],\n",
    "      [0, 0],\n",
    "])\n",
    "Y = np.array([\n",
    "      2,\n",
    "      1,\n",
    "      -2,\n",
    "      -1\n",
    "])\n",
    "# Init the parameters for the network\n",
    "clf = MultiLayerPerceptron(learning_rate = 0.1, num_iteration = 100000, is_Classifier = False)\n",
    "# Create the architecture backward\n",
    "clf.add_output_layer(num_neuron = 1, is_sigmoid=False)\n",
    "clf.add_hidden_layer(num_neuron = 2)\n",
    "clf.add_hidden_layer(num_neuron = 5)\n",
    "# Train the network\n",
    "clf.train(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected 2, got:  2.0\n",
      "Expected 1, got:  1.0\n",
      "Expected -2, got:  -2.0\n",
      "Expected -1, got:  -0.9999999999999998\n"
     ]
    }
   ],
   "source": [
    "print(\"Expected 2, got: \",clf.predict([1, 0]))\n",
    "print(\"Expected 1, got: \",clf.predict([0, 1]))\n",
    "print(\"Expected -2, got: \",clf.predict([1, 1]))\n",
    "print(\"Expected -1, got: \",clf.predict([0, 0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer0\n",
      "[0.6229016948897019, 0.7417869892607294, 0.7951935655656966]\n",
      "[0.9424502837770503, 0.7398985747399307, 0.922324996665417]\n",
      "Layer1\n",
      "[0.029005228283614737, 0.46562265437810535, 0.9433567169983137]\n",
      "[0.6489745531369242, 0.9009004917506227, 0.11320596465314436]\n",
      "[0.46906904778216374, 0.24657283261983032, 0.5437608592359304]\n",
      "[0.5739411879281008, 0.013114189588902203, 0.21672980046384815]\n",
      "Layer2\n",
      "[0.2794823660111103, 0.9163453718085519, 0.7657254516291417, 0.15960421235803823, 0.7971469914312045]\n",
      "Iteration 0 with error = 0.652476247467049\n",
      "Iteration 10000 with error = 1.1432780318744011e-26\n",
      "Iteration 20000 with error = 0.0\n",
      "Iteration 30000 with error = 0.0\n",
      "Iteration 40000 with error = 0.0\n",
      "Iteration 50000 with error = 0.0\n",
      "Iteration 60000 with error = 0.0\n",
      "Iteration 70000 with error = 0.0\n",
      "Iteration 80000 with error = 0.0\n",
      "Iteration 90000 with error = 0.0\n",
      "Layer0\n",
      "[0.5479002402027393, 0.6667855345737654, -2.5180206246879737]\n",
      "[0.49729347389547696, 0.2947417648583583, -1.0166073855153062]\n",
      "Layer1\n",
      "[-0.9098992213062894, -0.32931230344497886, 0.4116327231641602]\n",
      "[2.430042628339594, 1.3891715164087743, -2.1442765299708606]\n",
      "[1.4875655467464104, 0.519710182905767, -0.7685247376140738]\n",
      "[0.35000972430343297, -0.3272356083585742, -0.374615595014628]\n",
      "Layer2\n",
      "[-1.0356591576268974, 3.4066461372475114, 1.714669734392832, 0.04637483299661367, -0.15596113400657022]\n"
     ]
    }
   ],
   "source": [
    "X = np.array([\n",
    "      [1, 1],\n",
    "      [2, 2],\n",
    "      [3, 3]\n",
    "])\n",
    "Y = np.array([\n",
    "      1,\n",
    "      2,\n",
    "      3\n",
    "])\n",
    "# Init the parameters for the network\n",
    "clf = MultiLayerPerceptron(learning_rate = 0.1, num_iteration = 100000, is_Classifier = False)\n",
    "# Create the architecture backward\n",
    "clf.add_output_layer(num_neuron = 1, is_sigmoid=False)\n",
    "clf.add_hidden_layer(num_neuron = 4)\n",
    "clf.add_hidden_layer(num_neuron = 2)\n",
    "# Train the network\n",
    "clf.train(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected 1, got:  1.0\n",
      "Expected 2, got:  2.0\n",
      "Expected 3, got:  3.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Expected 1, got: \",clf.predict([1, 1]))\n",
    "print(\"Expected 2, got: \",clf.predict([2, 2]))\n",
    "print(\"Expected 3, got: \",clf.predict([3, 3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer0\n",
      "[0.6229016948897019, 0.7417869892607294, 0.7951935655656966]\n",
      "[0.9424502837770503, 0.7398985747399307, 0.922324996665417]\n",
      "[0.029005228283614737, 0.46562265437810535, 0.9433567169983137]\n",
      "Layer1\n",
      "[0.6489745531369242, 0.9009004917506227, 0.11320596465314436, 0.46906904778216374]\n",
      "[0.24657283261983032, 0.5437608592359304, 0.5739411879281008, 0.013114189588902203]\n",
      "[0.21672980046384815, 0.2794823660111103, 0.9163453718085519, 0.7657254516291417]\n",
      "[0.15960421235803823, 0.7971469914312045, 0.13876741839890316, 0.6174525204661166]\n",
      "Layer2\n",
      "[0.1266992325502697, 0.0017748622025346439, 0.8714047447242821, 0.2094563824951179, 0.21548116922473226]\n",
      "Iteration 0 with error = [1.42718673]\n",
      "Iteration 10000 with error = [0.00141865]\n",
      "Iteration 20000 with error = [0.00095781]\n",
      "Iteration 30000 with error = [0.00035247]\n",
      "Iteration 40000 with error = [0.0003229]\n",
      "Iteration 50000 with error = [0.00010845]\n",
      "Iteration 60000 with error = [7.11319543e-05]\n",
      "Iteration 70000 with error = [7.36919421e-05]\n",
      "Iteration 80000 with error = [0.00011589]\n",
      "Iteration 90000 with error = [9.05663469e-05]\n",
      "Layer0\n",
      "[array([5.05327527]), array([6.89774353]), array([-23.25872788])]\n",
      "[array([2.35428484]), array([2.89645098]), array([-10.35556086])]\n",
      "[array([1.30383856]), array([2.18019806]), array([-7.29839059])]\n",
      "Layer1\n",
      "[array([-1.72327214]), array([-0.15883893]), array([-0.82680435]), array([-2.09757733])]\n",
      "[array([-1.85046946]), array([-0.28848069]), array([-0.21766026]), array([-2.38091044])]\n",
      "[array([-9.14128751]), array([-3.70717635]), array([-2.43332558]), array([6.2716191])]\n",
      "[array([-2.59896979]), array([0.52714354]), array([-0.03169186]), array([-0.63513343])]\n",
      "Layer2\n",
      "[array([-0.37367746]), array([-0.17122941]), array([2.54433676]), array([-1.69025766]), array([-0.89591598])]\n"
     ]
    }
   ],
   "source": [
    "#GOOD\n",
    "X = np.concatenate([np.random.random((50,2)) * 0.9 + np.array([1, 1]), np.random.random((50,2)) * 0.9 + np.array([2, 2])])\n",
    "Y = np.concatenate([np.ones((50, 1)), np.ones((50, 1)) * -1.0])\n",
    "# Init the parameters for the network\n",
    "clf = MultiLayerPerceptron(learning_rate = 0.6, num_iteration = 100000, is_Classifier = False)\n",
    "# Create the architecture backward\n",
    "clf.add_output_layer(num_neuron = 1, is_sigmoid=False)\n",
    "clf.add_hidden_layer(num_neuron = 4)\n",
    "clf.add_hidden_layer(num_neuron = 3)\n",
    "# Train the network\n",
    "clf.train(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 310/310 [00:00<00:00, 699.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 with error = [0.66451613 0.66774194 0.66774194]\n",
      "Iteration 100 with error = [0.66451613 0.66774194 0.66774194]\n",
      "Iteration 200 with error = [0.66451613 0.66774194 0.66774194]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from PIL import Image, ImageOps \n",
    "import os \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as img\n",
    "\n",
    "def preprocess_dataset(paths, img_size):\n",
    "    list_X, list_Y, images = [], [], []\n",
    "    for path in paths:\n",
    "        images = images + [path + \"/\" + i for i in os.listdir(path)]\n",
    "\n",
    "    np.random.shuffle(images)\n",
    "    for filename in tqdm(images):\n",
    "        if \".\" in filename:\n",
    "            img = ImageOps.grayscale(Image.open(filename).resize(img_size))\n",
    "            array = np.asarray(img).flatten().tolist()\n",
    "            #plt.imshow(img, cmap='gray', vmin = 0, vmax = 255,interpolation='none')\n",
    "            #plt.show()\n",
    "\n",
    "            list_X.append(array)\n",
    "\n",
    "            if \"pizza\" in filename:\n",
    "                class_array = [1,0,0]#0\n",
    "            elif \"tarte aux pommes\" in filename:\n",
    "                class_array = [0,1,0]#1\n",
    "            elif \"tarte aux fraises\" in filename:\n",
    "                class_array = [0,0,1]#2\n",
    "            #On met 0,1,2 car nous sommes dans un modèle qui doit détecter plusieurs classes et on va renvoyer un tableau de proba (3 éléments)\n",
    "            #On fait un OneHotEncoder à la main car nous possédons 3 labels\n",
    "\n",
    "            list_Y.append(class_array)\n",
    "\n",
    "    X, Y = np.array(list_X), np.array(list_Y)\n",
    "    return X  / 255. ** 2, Y\n",
    "X, Y = preprocess_dataset([\"D:/Github/PROJET-ANNUEL-3IABD-SOLO/dataset/train/pizza\",\n",
    "                          \"D:/Github/PROJET-ANNUEL-3IABD-SOLO/dataset/train/tarte aux pommes\",\n",
    "                          \"D:/Github/PROJET-ANNUEL-3IABD-SOLO/dataset/train/tarte aux fraises\"\n",
    "                          ],\n",
    "                          (32, 32))\n",
    "\n",
    "# Init the parameters for the network\n",
    "clf = MultiLayerPerceptron( is_Classifier = True)\n",
    "# Create the architecture backward\n",
    "\n",
    "clf.add_output_layer(num_neuron = 3, is_sigmoid=True)\n",
    "clf.add_hidden_layer(num_neuron = 64, is_sigmoid=True)\n",
    "clf.add_hidden_layer(num_neuron = 64, is_sigmoid=False)\n",
    "\n",
    "#print(len(X[0]))\n",
    "#print(Y)\n",
    "# Train the network\n",
    "clf.train(X,Y,batchSize=20,epochs=400,learning_rate =5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtest, Ytest = preprocess_dataset([\"D:/Github/PROJET-ANNUEL-3IABD-SOLO/dataset/test/pizza\",\n",
    "                          \"D:/Github/PROJET-ANNUEL-3IABD-SOLO/dataset/test/tarte aux pommes\",\n",
    "                          \"D:/Github/PROJET-ANNUEL-3IABD-SOLO/dataset/test/tarte aux fraises\",],\n",
    "                          (16, 16))\n",
    "clf.validate(Xtest,Ytest)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e476f5aa093f7c2b5866145438d709a47dda12c45cd5b462f3d982568c4b540d"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "268a2b46a2a1fd7cf1d0a6e4207746d31a1df269f74a36b08244a1a810a7653e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
