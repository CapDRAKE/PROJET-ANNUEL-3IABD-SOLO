{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "\n",
    "\n",
    "class Neuron():\n",
    "    '''\n",
    "        A conceptual Neuron hat can be trained using a \n",
    "        fit and predict methodology, without any library\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, position_in_layer, is_output_neuron=False,is_sigmoid=True):\n",
    "        self.weights = []\n",
    "        self.inputs = []\n",
    "        self.output = None\n",
    "        self.is_sigmoid=is_sigmoid\n",
    "\n",
    "        # This is used for the backpropagation update\n",
    "        self.updated_weights = []\n",
    "        # This is used to know how to update the weights\n",
    "        self.is_output_neuron = is_output_neuron\n",
    "        # This delta is used for the update at the backpropagation\n",
    "        self.delta = None\n",
    "        # This is used for the backpropagation update\n",
    "        self.position_in_layer = position_in_layer \n",
    "        \n",
    "    def attach_to_output(self, neurons):\n",
    "        '''\n",
    "            Helper function to store the reference of the other neurons\n",
    "            To this particular neuron (used for backpropagation)\n",
    "        '''\n",
    "        \n",
    "        self.output_neurons = neurons\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        '''\n",
    "            simple sigmoid function (logistic) used for the activation\n",
    "        '''\n",
    "        return 1 / (1 + math.exp(-x))\n",
    "    \n",
    "    def activation(self,x):\n",
    "        if self.is_sigmoid:\n",
    "            return self.sigmoid(x)\n",
    "        else :\n",
    "            return x\n",
    "    \n",
    "    def derivation(self,x):\n",
    "        if self.is_sigmoid:\n",
    "            return x*(1-x)\n",
    "        else :\n",
    "            return 1\n",
    "    \n",
    "    def init_weights(self, num_input):\n",
    "        '''\n",
    "            This is used to setup the weights when we know how many inputs there is for\n",
    "            a given neuron\n",
    "        '''\n",
    "        \n",
    "        # Randomly initalize the weights\n",
    "        for i in range(num_input+1):\n",
    "            self.weights.append(random.uniform(0,1))\n",
    "        \n",
    "    def predict(self, row):\n",
    "        '''\n",
    "            Given a row of data it will predict what the output should be for\n",
    "            this given neuron. We can have many input, but only one output for a neuron\n",
    "        '''\n",
    "        \n",
    "        # Reset the inputs\n",
    "        self.inputs = []\n",
    "        \n",
    "        # We iterate over the weights and the features in the given row\n",
    "        activation = 0\n",
    "        for weight, feature in zip(self.weights, row):\n",
    "            self.inputs.append(feature)\n",
    "            activation = activation + weight*feature\n",
    "            \n",
    "        \n",
    "        self.output = self.activation(activation)\n",
    "        return self.output\n",
    "    \n",
    "        \n",
    "            \n",
    "    def update_neuron(self):\n",
    "        '''\n",
    "            Will update a given neuron weights by replacing the current weights\n",
    "            with those used during the backpropagation. This need to be done at the end of the\n",
    "            backpropagation\n",
    "        '''\n",
    "        \n",
    "        self.weights = []\n",
    "        for new_weight in self.updated_weights:\n",
    "            self.weights.append(new_weight)\n",
    "    \n",
    "    def calculate_update(self, learning_rate, target):\n",
    "        '''\n",
    "            This function will calculate the updated weights for this neuron. It will first calculate\n",
    "            the right delta (depending if this neuron is a ouput or a hidden neuron), then it will\n",
    "            calculate the right updated_weights. It will not overwrite the weights yet as they are needed\n",
    "            for other update in the backpropagation algorithm.\n",
    "        '''\n",
    "        \n",
    "        if self.is_output_neuron:\n",
    "            # Calculate the delta for the output\n",
    "            self.delta = (self.output - target)*self.derivation(self.output)\n",
    "        else:\n",
    "            # Calculate the delta\n",
    "            delta_sum = 0\n",
    "            # this is to know which weights this neuron is contributing in the output layer\n",
    "            cur_weight_index = self.position_in_layer \n",
    "            for output_neuron in self.output_neurons:\n",
    "                delta_sum = delta_sum + (output_neuron.delta * output_neuron.weights[cur_weight_index])\n",
    "\n",
    "            # Update this neuron delta\n",
    "            self.delta = delta_sum*self.derivation(self.output)\n",
    "            \n",
    "            \n",
    "        # Reset the update weights\n",
    "        self.updated_weights = []\n",
    "        \n",
    "        # Iterate over each weight and update them\n",
    "        for cur_weight, cur_input in zip(self.weights, self.inputs):\n",
    "            gradient = self.delta*cur_input\n",
    "            new_weight = cur_weight - learning_rate*gradient\n",
    "            self.updated_weights.append(new_weight)\n",
    "        \n",
    "    def toString(self):\n",
    "        print (self.weights)\n",
    "        \n",
    "        \n",
    "class Layer():\n",
    "    '''\n",
    "        Layer is modelizing a layer in the fully-connected-feedforward neural network architecture.\n",
    "        It will play the role of connecting everything together inside and will be doing the backpropagation \n",
    "        update.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, num_neuron, is_output_layer = False,is_sigmoid=True):\n",
    "        \n",
    "        # Will create that much neurons in this layer\n",
    "        self.is_output_layer = is_output_layer\n",
    "        self.neurons = []\n",
    "        for i in range(num_neuron):\n",
    "            # Create neuron\n",
    "            neuron = Neuron(i,  is_output_neuron=is_output_layer,is_sigmoid=is_sigmoid)\n",
    "            self.neurons.append(neuron)\n",
    "    \n",
    "    def toString(self):\n",
    "        for in_neuron in self.neurons:\n",
    "            in_neuron.toString()\n",
    "    \n",
    "    def attach(self, layer):\n",
    "        '''\n",
    "            This function attach the neurons from this layer to another one\n",
    "            This is needed for the backpropagation algorithm\n",
    "        '''\n",
    "        # Iterate over the neurons in the current layer and attach \n",
    "        # them to the next layer\n",
    "        for in_neuron in self.neurons:\n",
    "            in_neuron.attach_to_output(layer.neurons)\n",
    "            \n",
    "    def init_layer(self, num_input):\n",
    "        '''\n",
    "            This will initialize the weights of each neuron in the layer.\n",
    "            By giving the right num_input it will spawn the right number of weights\n",
    "        '''\n",
    "        \n",
    "        # Iterate over each of the neuron and initialize\n",
    "        # the weights that connect with the previous layer\n",
    "        for neuron in self.neurons:\n",
    "            neuron.init_weights(num_input)\n",
    "    \n",
    "    def predict(self, row):\n",
    "        '''\n",
    "            This will calcualte the activations for the full layer given the row of data \n",
    "            streaming in.\n",
    "        '''\n",
    "        row_copy=row.copy()\n",
    "        row_copy.append(1) # need to add the bias\n",
    "        activations = [neuron.predict(row_copy) for neuron in self.neurons]\n",
    "        return activations\n",
    "\n",
    "        \n",
    "class MultiLayerPerceptron():\n",
    "    '''\n",
    "        We will be creating the multi-layer perceptron with only two layer:\n",
    "        an input layer, a perceptrons layer and a one neuron output layer which does binary classification\n",
    "    '''\n",
    "    def __init__(self, learning_rate = 0.01, num_iteration = 100,is_oneorzero=True):\n",
    "        \n",
    "        # Layers\n",
    "        self.layers = []\n",
    "                \n",
    "        # Training parameters\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_iteration = num_iteration\n",
    "        \n",
    "        self.is_oneorzero=is_oneorzero\n",
    "        \n",
    "    def toString(self):\n",
    "         for i,layer in enumerate( self.layers):\n",
    "            print (\"layer \" +str(i))\n",
    "            layer.toString()\n",
    "            \n",
    "    def add_output_layer(self, num_neuron,is_sigmoid=True):\n",
    "        '''\n",
    "            This helper function will create a new output layer and add it to the architecture\n",
    "        '''\n",
    "        self.layers.insert(0, Layer(num_neuron, is_output_layer = True,is_sigmoid=is_sigmoid))\n",
    "    \n",
    "    def add_hidden_layer(self, num_neuron):\n",
    "        '''\n",
    "            This helper function will create a new hidden layer, add it to the architecture\n",
    "            and finally attach it to the front of the architecture\n",
    "        '''\n",
    "        # Create an hidden layer\n",
    "        hidden_layer = Layer(num_neuron)\n",
    "        # Attach the last added layer to this new layer\n",
    "        hidden_layer.attach(self.layers[0])\n",
    "        # Add this layers to the architecture\n",
    "        self.layers.insert(0, hidden_layer)\n",
    "        \n",
    "    def update_layers(self, target):\n",
    "        '''\n",
    "            Will update all the layers by calculating the updated weights and then updating \n",
    "            the weights all at once when the new weights are found.\n",
    "        '''\n",
    "        # Iterate over each of the layer in reverse order\n",
    "        # to calculate the updated weights\n",
    "        for layer in reversed(self.layers):\n",
    "                           \n",
    "            # Calculate update the hidden layer\n",
    "            for neuron in layer.neurons:\n",
    "                neuron.calculate_update(self.learning_rate, target)  \n",
    "        \n",
    "        # Iterate over each of the layer in normal order\n",
    "        # to update the weights\n",
    "        for layer in self.layers:\n",
    "            for neuron in layer.neurons:\n",
    "                neuron.update_neuron()\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        '''\n",
    "            Main training function of the neural network algorithm. This will make use of backpropagation.\n",
    "            It will use stochastic gradient descent by selecting one row at random from the dataset and \n",
    "            use predict to calculate the error. The error will then be backpropagated and new weights calculated.\n",
    "            Once all the new weights are calculated, the whole network weights will be updated\n",
    "        '''\n",
    "        num_row = len(X)\n",
    "        num_feature = len(X[0]) # Here we assume that we have a rectangular matrix\n",
    "        \n",
    "        # Init the weights throughout each of the layer\n",
    "        self.layers[0].init_layer(num_feature)\n",
    "\n",
    "        for i in range(1, len(self.layers)):\n",
    "            num_input = len(self.layers[i-1].neurons)\n",
    "            self.layers[i].init_layer(num_input)\n",
    "        self.toString()\n",
    "        # Launch the training algorithm\n",
    "        for i in range(self.num_iteration):\n",
    "            \n",
    "            # Stochastic Gradient Descent\n",
    "            r_i = random.randint(0,num_row-1)\n",
    "            row = X[r_i] # take the random sample from the dataset\n",
    "            yhat = self.predict(row)\n",
    "            target = y[r_i]\n",
    "            \n",
    "        \n",
    "            # Update the layers using backpropagation   \n",
    "            self.update_layers(target)\n",
    "            \n",
    "            # At every 100 iteration we calculate the error\n",
    "            # on the whole training set\n",
    "            if i % 1000 == 0:\n",
    "                \n",
    "                total_error = 0\n",
    "                for r_i in range(num_row):\n",
    "                  \n",
    "                    row = X[r_i]\n",
    "                    yhat = self.predict(row)\n",
    "                    #print(\"input : \")\n",
    "                    #print(row)\n",
    "                    #print(\"output : \")\n",
    "                    #print(yhat)\n",
    "                    \n",
    "                    error = (y[r_i] - yhat)\n",
    "                    total_error = total_error + error**2\n",
    "                mean_error = total_error/num_row\n",
    "                print(f\"Iteration {i} with error = {mean_error}\")\n",
    "        self.toString()\n",
    "        \n",
    "    \n",
    "    def predict(self, row):\n",
    "        '''\n",
    "            Prediction function that will take a row of input and give back the output\n",
    "            of the whole neural network.\n",
    "        '''\n",
    "        \n",
    "        # Gather all the activation in the hidden layer\n",
    "        \n",
    "        activations = self.layers[0].predict(row)\n",
    "        for i in range(1, len(self.layers)):\n",
    "            activations = self.layers[i].predict(activations)\n",
    "\n",
    "        outputs = []\n",
    "        for activation in activations:                        \n",
    "            if self.is_oneorzero:\n",
    "                # Decide if we output a 1 or 0\n",
    "                if activation >= 0.5:\n",
    "                    outputs.append(1.0)\n",
    "                else:\n",
    "                    outputs.append(0.0)\n",
    "            else:\n",
    "                outputs.append(activation)\n",
    "                           \n",
    "        # We currently have only One output allowed\n",
    "        return outputs[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 0\n",
      "[0.9932190809171046, 0.5493626041649108, 0.13071100414115666]\n",
      "[0.19015261514620696, 0.5971604344605017, 0.13532046255142216]\n",
      "layer 1\n",
      "[0.012165141081967423, 0.9936933857705729, 0.13700393567563363]\n",
      "[0.7219319156445897, 0.38103895780859187, 0.6863465526890987]\n",
      "[0.8421308599883414, 0.5221985215724014, 0.8801825746216673]\n",
      "layer 2\n",
      "[0.10247046358055034, 0.6068793787454135, 0.1749519373949595, 0.6031217336149657]\n",
      "Iteration 0 with error = 0.5\n",
      "Iteration 1000 with error = 0.5\n",
      "Iteration 2000 with error = 0.5\n",
      "Iteration 3000 with error = 0.5\n",
      "Iteration 4000 with error = 0.25\n",
      "Iteration 5000 with error = 0.5\n",
      "Iteration 6000 with error = 0.5\n",
      "Iteration 7000 with error = 0.5\n",
      "Iteration 8000 with error = 0.5\n",
      "Iteration 9000 with error = 0.5\n",
      "Iteration 10000 with error = 0.5\n",
      "Iteration 11000 with error = 0.5\n",
      "Iteration 12000 with error = 0.5\n",
      "Iteration 13000 with error = 0.75\n",
      "Iteration 14000 with error = 0.5\n",
      "Iteration 15000 with error = 0.25\n",
      "Iteration 16000 with error = 0.25\n",
      "Iteration 17000 with error = 0.25\n",
      "Iteration 18000 with error = 0.25\n",
      "Iteration 19000 with error = 0.25\n",
      "Iteration 20000 with error = 0.0\n",
      "Iteration 21000 with error = 0.0\n",
      "Iteration 22000 with error = 0.0\n",
      "Iteration 23000 with error = 0.0\n",
      "Iteration 24000 with error = 0.0\n",
      "Iteration 25000 with error = 0.0\n",
      "Iteration 26000 with error = 0.0\n",
      "Iteration 27000 with error = 0.0\n",
      "Iteration 28000 with error = 0.0\n",
      "Iteration 29000 with error = 0.0\n",
      "Iteration 30000 with error = 0.0\n",
      "Iteration 31000 with error = 0.0\n",
      "Iteration 32000 with error = 0.0\n",
      "Iteration 33000 with error = 0.0\n",
      "Iteration 34000 with error = 0.0\n",
      "Iteration 35000 with error = 0.0\n",
      "Iteration 36000 with error = 0.0\n",
      "Iteration 37000 with error = 0.0\n",
      "Iteration 38000 with error = 0.0\n",
      "Iteration 39000 with error = 0.0\n",
      "Iteration 40000 with error = 0.0\n",
      "Iteration 41000 with error = 0.0\n",
      "Iteration 42000 with error = 0.0\n",
      "Iteration 43000 with error = 0.0\n",
      "Iteration 44000 with error = 0.0\n",
      "Iteration 45000 with error = 0.0\n",
      "Iteration 46000 with error = 0.0\n",
      "Iteration 47000 with error = 0.0\n",
      "Iteration 48000 with error = 0.0\n",
      "Iteration 49000 with error = 0.0\n",
      "Iteration 50000 with error = 0.0\n",
      "Iteration 51000 with error = 0.0\n",
      "Iteration 52000 with error = 0.0\n",
      "Iteration 53000 with error = 0.0\n",
      "Iteration 54000 with error = 0.0\n",
      "Iteration 55000 with error = 0.0\n",
      "Iteration 56000 with error = 0.0\n",
      "Iteration 57000 with error = 0.0\n",
      "Iteration 58000 with error = 0.0\n",
      "Iteration 59000 with error = 0.0\n",
      "Iteration 60000 with error = 0.0\n",
      "Iteration 61000 with error = 0.0\n",
      "Iteration 62000 with error = 0.0\n",
      "Iteration 63000 with error = 0.0\n",
      "Iteration 64000 with error = 0.0\n",
      "Iteration 65000 with error = 0.0\n",
      "Iteration 66000 with error = 0.0\n",
      "Iteration 67000 with error = 0.0\n",
      "Iteration 68000 with error = 0.0\n",
      "Iteration 69000 with error = 0.0\n",
      "Iteration 70000 with error = 0.0\n",
      "Iteration 71000 with error = 0.0\n",
      "Iteration 72000 with error = 0.0\n",
      "Iteration 73000 with error = 0.0\n",
      "Iteration 74000 with error = 0.0\n",
      "Iteration 75000 with error = 0.0\n",
      "Iteration 76000 with error = 0.0\n",
      "Iteration 77000 with error = 0.0\n",
      "Iteration 78000 with error = 0.0\n",
      "Iteration 79000 with error = 0.0\n",
      "Iteration 80000 with error = 0.0\n",
      "Iteration 81000 with error = 0.0\n",
      "Iteration 82000 with error = 0.0\n",
      "Iteration 83000 with error = 0.0\n",
      "Iteration 84000 with error = 0.0\n",
      "Iteration 85000 with error = 0.0\n",
      "Iteration 86000 with error = 0.0\n",
      "Iteration 87000 with error = 0.0\n",
      "Iteration 88000 with error = 0.0\n",
      "Iteration 89000 with error = 0.0\n",
      "Iteration 90000 with error = 0.0\n",
      "Iteration 91000 with error = 0.0\n",
      "Iteration 92000 with error = 0.0\n",
      "Iteration 93000 with error = 0.0\n",
      "Iteration 94000 with error = 0.0\n",
      "Iteration 95000 with error = 0.0\n",
      "Iteration 96000 with error = 0.0\n",
      "Iteration 97000 with error = 0.0\n",
      "Iteration 98000 with error = 0.0\n",
      "Iteration 99000 with error = 0.0\n",
      "layer 0\n",
      "[5.360003228007821, 5.399074034971362, -2.1231710731023092]\n",
      "[3.1518471787370363, 3.161997070758369, -4.823956994341526]\n",
      "layer 1\n",
      "[-4.699996181882465, 5.557415557388883, 1.8532313104200355]\n",
      "[3.9006170646112275, -4.031231337516267, -1.717205603191995]\n",
      "[1.2751101832680432, -0.9244782662446184, -0.19245126516066374]\n",
      "layer 2\n",
      "[-8.009887224924965, 6.2387778524826585, 1.73543184753649, -0.4070325811853751]\n"
     ]
    }
   ],
   "source": [
    "# XOR function (one or the other but not both)\n",
    "X = [[0,0], [0,1], [1,0], [1,1]]\n",
    "y = [0, 1, 1, 0]\n",
    "\n",
    "# Init the parameters for the network\n",
    "clf = MultiLayerPerceptron(learning_rate = 0.2, num_iteration = 100000)\n",
    "# Create the architecture backward\n",
    "clf.add_output_layer(num_neuron = 1)\n",
    "clf.add_hidden_layer(num_neuron = 3)\n",
    "clf.add_hidden_layer(num_neuron = 2)\n",
    "# Train the network\n",
    "clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected 0.0, got:  0.0\n",
      "Expected 1.0, got:  1.0\n",
      "Expected 1.0, got:  1.0\n",
      "Expected 0.0, got:  0.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Expected 0.0, got: \",clf.predict([0,0]))\n",
    "print(\"Expected 1.0, got: \",clf.predict([0,1]))\n",
    "print(\"Expected 1.0, got: \",clf.predict([1,0]))\n",
    "print(\"Expected 0.0, got: \",clf.predict([1,1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 0\n",
      "[0.020743520161728313, 0.5896787740764242]\n",
      "[0.9453843251714572, 0.9421535700448487]\n",
      "layer 1\n",
      "[0.4991580554252365, 0.43923126765566833, 0.967611664295238]\n",
      "Iteration 0 with error = 0.5686782983968527\n",
      "Iteration 1000 with error = 0.121782759032863\n",
      "Iteration 2000 with error = 0.10604920065451634\n",
      "Iteration 3000 with error = 0.09213405781901274\n",
      "Iteration 4000 with error = 0.08246760209497692\n",
      "Iteration 5000 with error = 0.07515772354087263\n",
      "Iteration 6000 with error = 0.08855995828114299\n",
      "Iteration 7000 with error = 0.07303399508851037\n",
      "Iteration 8000 with error = 0.07233515769013406\n",
      "Iteration 9000 with error = 0.061177687618897254\n",
      "Iteration 10000 with error = 0.05887856559408364\n",
      "Iteration 11000 with error = 0.05808873515542182\n",
      "Iteration 12000 with error = 0.05531693955293762\n",
      "Iteration 13000 with error = 0.05490270477205119\n",
      "Iteration 14000 with error = 0.05299016977165544\n",
      "Iteration 15000 with error = 0.05247381264218028\n",
      "Iteration 16000 with error = 0.061710144433522886\n",
      "Iteration 17000 with error = 0.0530520512720744\n",
      "Iteration 18000 with error = 0.053211451536926496\n",
      "Iteration 19000 with error = 0.05085342779029677\n",
      "Iteration 20000 with error = 0.04934454328514524\n",
      "Iteration 21000 with error = 0.051225309032669204\n",
      "Iteration 22000 with error = 0.04977097299188299\n",
      "Iteration 23000 with error = 0.048169133413849374\n",
      "Iteration 24000 with error = 0.0500475701364371\n",
      "Iteration 25000 with error = 0.05381513852858836\n",
      "Iteration 26000 with error = 0.049438338020358207\n",
      "Iteration 27000 with error = 0.04745135111003757\n",
      "Iteration 28000 with error = 0.046848272337064394\n",
      "Iteration 29000 with error = 0.04881954867425146\n",
      "Iteration 30000 with error = 0.054654410308577685\n",
      "Iteration 31000 with error = 0.05642440855589894\n",
      "Iteration 32000 with error = 0.05554754333920179\n",
      "Iteration 33000 with error = 0.045878758693621326\n",
      "Iteration 34000 with error = 0.07241297325044592\n",
      "Iteration 35000 with error = 0.045562321148611785\n",
      "Iteration 36000 with error = 0.07077445350679383\n",
      "Iteration 37000 with error = 0.046003422252899384\n",
      "Iteration 38000 with error = 0.04873777942755689\n",
      "Iteration 39000 with error = 0.04576930394453129\n",
      "Iteration 40000 with error = 0.045365354405333874\n",
      "Iteration 41000 with error = 0.04575752037768363\n",
      "Iteration 42000 with error = 0.05530205493326889\n",
      "Iteration 43000 with error = 0.056314471701496914\n",
      "Iteration 44000 with error = 0.04783734834774289\n",
      "Iteration 45000 with error = 0.04527157453238312\n",
      "Iteration 46000 with error = 0.053289218421442346\n",
      "Iteration 47000 with error = 0.046220934605680365\n",
      "Iteration 48000 with error = 0.04499691586396328\n",
      "Iteration 49000 with error = 0.05765950600307022\n",
      "Iteration 50000 with error = 0.045991880721332146\n",
      "Iteration 51000 with error = 0.050570060579792846\n",
      "Iteration 52000 with error = 0.04447106236707931\n",
      "Iteration 53000 with error = 0.044626200686233915\n",
      "Iteration 54000 with error = 0.04461662973163693\n",
      "Iteration 55000 with error = 0.04570378364464225\n",
      "Iteration 56000 with error = 0.04392376973611951\n",
      "Iteration 57000 with error = 0.044000461611950814\n",
      "Iteration 58000 with error = 0.046184431221035\n",
      "Iteration 59000 with error = 0.04390982115859284\n",
      "Iteration 60000 with error = 0.04736014091612099\n",
      "Iteration 61000 with error = 0.04382903640371052\n",
      "Iteration 62000 with error = 0.04384232728915081\n",
      "Iteration 63000 with error = 0.04645498631794534\n",
      "Iteration 64000 with error = 0.04526371708306213\n",
      "Iteration 65000 with error = 0.04660011664971312\n",
      "Iteration 66000 with error = 0.0446873257709405\n",
      "Iteration 67000 with error = 0.05501940009881545\n",
      "Iteration 68000 with error = 0.04621033995181271\n",
      "Iteration 69000 with error = 0.04375049338899798\n",
      "Iteration 70000 with error = 0.045285242553295836\n",
      "Iteration 71000 with error = 0.043743995696894855\n",
      "Iteration 72000 with error = 0.046415758343079566\n",
      "Iteration 73000 with error = 0.044247365716605924\n",
      "Iteration 74000 with error = 0.043285494724506934\n",
      "Iteration 75000 with error = 0.04737172109915066\n",
      "Iteration 76000 with error = 0.043250037219619376\n",
      "Iteration 77000 with error = 0.043434186996426004\n",
      "Iteration 78000 with error = 0.043882733210656145\n",
      "Iteration 79000 with error = 0.0435519935516975\n",
      "Iteration 80000 with error = 0.048723387568862415\n",
      "Iteration 81000 with error = 0.04527359876034284\n",
      "Iteration 82000 with error = 0.043130822754353544\n",
      "Iteration 83000 with error = 0.04457183378331043\n",
      "Iteration 84000 with error = 0.0450944045735473\n",
      "Iteration 85000 with error = 0.04315619503461463\n",
      "Iteration 86000 with error = 0.04328994482085699\n",
      "Iteration 87000 with error = 0.043212249396260984\n",
      "Iteration 88000 with error = 0.04761701949019597\n",
      "Iteration 89000 with error = 0.050952222297422765\n",
      "Iteration 90000 with error = 0.0440363850402458\n",
      "Iteration 91000 with error = 0.043319193432427176\n",
      "Iteration 92000 with error = 0.04472287518459914\n",
      "Iteration 93000 with error = 0.04490205307941771\n",
      "Iteration 94000 with error = 0.04342014899683889\n",
      "Iteration 95000 with error = 0.045161786080363746\n",
      "Iteration 96000 with error = 0.0491096673535776\n",
      "Iteration 97000 with error = 0.054635743944699534\n",
      "Iteration 98000 with error = 0.04421304890816302\n",
      "Iteration 99000 with error = 0.04332776183895257\n",
      "layer 0\n",
      "[3.9776717243758077, -1.2295327279390564]\n",
      "[5.1011942534580825, -3.8907976734874645]\n",
      "layer 1\n",
      "[1.0595880711424945, 2.9934110367806888, -1.281388355253015]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# 1. Creation of test data set\n",
    "\n",
    "X = [\n",
    "      [1],\n",
    "      [2],\n",
    "      [3]\n",
    "]\n",
    "Y = [\n",
    "      2,\n",
    "      3,\n",
    "      2.5\n",
    "]\n",
    "\n",
    "\n",
    "# Init the parameters for the network\n",
    "clf = MultiLayerPerceptron(learning_rate = 0.05, num_iteration = 100000,is_oneorzero=False)\n",
    "# Create the architecture backward\n",
    "clf.add_output_layer(num_neuron = 1,is_sigmoid=False)\n",
    "clf.add_hidden_layer(num_neuron = 2)\n",
    "#clf.add_hidden_layer(num_neuron = 3)\n",
    "# Train the network\n",
    "clf.fit(X,Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected 2, got:  2.0204525631680434\n",
      "Expected 3, got:  2.7649176235996373\n",
      "Expected 2.5, got:  2.771553861315999\n"
     ]
    }
   ],
   "source": [
    "print(\"Expected 2, got: \",clf.predict([1]))\n",
    "print(\"Expected 3, got: \",clf.predict([2]))\n",
    "print(\"Expected 2.5, got: \",clf.predict([3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 0\n",
      "[0.9869756687859785, 0.04775519615064472, 0.17814126859029866, 0.5655745733123076]\n",
      "[0.7068034781923735, 0.49865183582326256, 0.477975242109108, 0.9246616192027135]\n",
      "layer 1\n",
      "[0.43670428153579044, 0.621745150584237, 0.43081587416149114]\n",
      "[0.6584891657945031, 0.3006723263818283, 0.978093824413422]\n",
      "[0.069756497966588, 0.36693529461890184, 0.042098530145897706]\n",
      "layer 2\n",
      "[0.5032841159472845, 0.41325408870688707, 0.4232173419343527, 0.6841855159652771]\n",
      "Iteration 0 with error = 0.5\n",
      "Iteration 1000 with error = 0.5\n",
      "Iteration 2000 with error = 0.5\n",
      "Iteration 3000 with error = 0.5\n",
      "Iteration 4000 with error = 0.5\n",
      "Iteration 5000 with error = 0.5\n",
      "Iteration 6000 with error = 0.3333333333333333\n",
      "Iteration 7000 with error = 0.08333333333333333\n",
      "Iteration 8000 with error = 0.08333333333333333\n",
      "Iteration 9000 with error = 0.08333333333333333\n",
      "Iteration 10000 with error = 0.08333333333333333\n",
      "Iteration 11000 with error = 0.08333333333333333\n",
      "Iteration 12000 with error = 0.08333333333333333\n",
      "Iteration 13000 with error = 0.08333333333333333\n",
      "Iteration 14000 with error = 0.08333333333333333\n",
      "Iteration 15000 with error = 0.08333333333333333\n",
      "Iteration 16000 with error = 0.08333333333333333\n",
      "Iteration 17000 with error = 0.08333333333333333\n",
      "Iteration 18000 with error = 0.08333333333333333\n",
      "Iteration 19000 with error = 0.08333333333333333\n",
      "Iteration 20000 with error = 0.08333333333333333\n",
      "Iteration 21000 with error = 0.08333333333333333\n",
      "Iteration 22000 with error = 0.08333333333333333\n",
      "Iteration 23000 with error = 0.08333333333333333\n",
      "Iteration 24000 with error = 0.08333333333333333\n",
      "Iteration 25000 with error = 0.08333333333333333\n",
      "Iteration 26000 with error = 0.08333333333333333\n",
      "Iteration 27000 with error = 0.08333333333333333\n",
      "Iteration 28000 with error = 0.08333333333333333\n",
      "Iteration 29000 with error = 0.08333333333333333\n",
      "Iteration 30000 with error = 0.08333333333333333\n",
      "Iteration 31000 with error = 0.08333333333333333\n",
      "Iteration 32000 with error = 0.08333333333333333\n",
      "Iteration 33000 with error = 0.08333333333333333\n",
      "Iteration 34000 with error = 0.08333333333333333\n",
      "Iteration 35000 with error = 0.08333333333333333\n",
      "Iteration 36000 with error = 0.08333333333333333\n",
      "Iteration 37000 with error = 0.08333333333333333\n",
      "Iteration 38000 with error = 0.08333333333333333\n",
      "Iteration 39000 with error = 0.08333333333333333\n",
      "Iteration 40000 with error = 0.08333333333333333\n",
      "Iteration 41000 with error = 0.08333333333333333\n",
      "Iteration 42000 with error = 0.08333333333333333\n",
      "Iteration 43000 with error = 0.08333333333333333\n",
      "Iteration 44000 with error = 0.08333333333333333\n",
      "Iteration 45000 with error = 0.08333333333333333\n",
      "Iteration 46000 with error = 0.08333333333333333\n",
      "Iteration 47000 with error = 0.08333333333333333\n",
      "Iteration 48000 with error = 0.08333333333333333\n",
      "Iteration 49000 with error = 0.08333333333333333\n",
      "Iteration 50000 with error = 0.08333333333333333\n",
      "Iteration 51000 with error = 0.08333333333333333\n",
      "Iteration 52000 with error = 0.08333333333333333\n",
      "Iteration 53000 with error = 0.08333333333333333\n",
      "Iteration 54000 with error = 0.08333333333333333\n",
      "Iteration 55000 with error = 0.08333333333333333\n",
      "Iteration 56000 with error = 0.08333333333333333\n",
      "Iteration 57000 with error = 0.08333333333333333\n",
      "Iteration 58000 with error = 0.08333333333333333\n",
      "Iteration 59000 with error = 0.08333333333333333\n",
      "Iteration 60000 with error = 0.08333333333333333\n",
      "Iteration 61000 with error = 0.08333333333333333\n",
      "Iteration 62000 with error = 0.08333333333333333\n",
      "Iteration 63000 with error = 0.08333333333333333\n",
      "Iteration 64000 with error = 0.08333333333333333\n",
      "Iteration 65000 with error = 0.08333333333333333\n",
      "Iteration 66000 with error = 0.08333333333333333\n",
      "Iteration 67000 with error = 0.08333333333333333\n",
      "Iteration 68000 with error = 0.08333333333333333\n",
      "Iteration 69000 with error = 0.08333333333333333\n",
      "Iteration 70000 with error = 0.08333333333333333\n",
      "Iteration 71000 with error = 0.08333333333333333\n",
      "Iteration 72000 with error = 0.08333333333333333\n",
      "Iteration 73000 with error = 0.08333333333333333\n",
      "Iteration 74000 with error = 0.08333333333333333\n",
      "Iteration 75000 with error = 0.08333333333333333\n",
      "Iteration 76000 with error = 0.08333333333333333\n",
      "Iteration 77000 with error = 0.08333333333333333\n",
      "Iteration 78000 with error = 0.08333333333333333\n",
      "Iteration 79000 with error = 0.08333333333333333\n",
      "Iteration 80000 with error = 0.08333333333333333\n",
      "Iteration 81000 with error = 0.08333333333333333\n",
      "Iteration 82000 with error = 0.08333333333333333\n",
      "Iteration 83000 with error = 0.08333333333333333\n",
      "Iteration 84000 with error = 0.08333333333333333\n",
      "Iteration 85000 with error = 0.08333333333333333\n",
      "Iteration 86000 with error = 0.08333333333333333\n",
      "Iteration 87000 with error = 0.08333333333333333\n",
      "Iteration 88000 with error = 0.08333333333333333\n",
      "Iteration 89000 with error = 0.08333333333333333\n",
      "Iteration 90000 with error = 0.08333333333333333\n",
      "Iteration 91000 with error = 0.08333333333333333\n",
      "Iteration 92000 with error = 0.08333333333333333\n",
      "Iteration 93000 with error = 0.08333333333333333\n",
      "Iteration 94000 with error = 0.08333333333333333\n",
      "Iteration 95000 with error = 0.08333333333333333\n",
      "Iteration 96000 with error = 0.08333333333333333\n",
      "Iteration 97000 with error = 0.08333333333333333\n",
      "Iteration 98000 with error = 0.08333333333333333\n",
      "Iteration 99000 with error = 0.08333333333333333\n",
      "layer 0\n",
      "[4.768041677234483, 4.782975015028588, 2.4052598829018597, -5.618812058081693]\n",
      "[5.04754535471704, 5.063532997502329, 2.5458778275175, -5.989034576724573]\n",
      "layer 1\n",
      "[6.504680308906568, 7.026042544272151, -5.307184327768841]\n",
      "[-2.8779504263473807, -3.5267690354196333, -0.06916521595255701]\n",
      "[-3.020921224115573, -2.9736578460994587, -0.39929845235919736]\n",
      "layer 2\n",
      "[6.735695721716577, -2.5521997628758655, -2.4142159951960362, -5.186882135037665]\n"
     ]
    }
   ],
   "source": [
    "# AND,OR,XOR function (one or the other but not both)\n",
    "X = [[0,0,0], [0,1,0], [1,0,0], [1,1,0],[0,0,1], [0,1,1], [1,0,1], [1,1,1],[0,0,2], [0,1,2], [1,0,2], [1,1,2]]\n",
    "y = [0, 0, 0, 1,0, 1, 1, 1,0, 1, 1, 0]\n",
    "\n",
    "# Init the parameters for the network\n",
    "clf = MultiLayerPerceptron(learning_rate = 0.4, num_iteration = 100000)\n",
    "# Create the architecture backward\n",
    "clf.add_output_layer(num_neuron = 1)\n",
    "clf.add_hidden_layer(num_neuron = 3)\n",
    "clf.add_hidden_layer(num_neuron = 2)\n",
    "# Train the network\n",
    "clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected 0, got:  0.0\n",
      "Expected 1, got:  1.0\n",
      "Expected 1, got:  1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Expected 0, got: \",clf.predict([0,0,0]))\n",
    "print(\"Expected 1, got: \",clf.predict([1,0,1]))\n",
    "print(\"Expected 1, got: \",clf.predict([0,1,2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "268a2b46a2a1fd7cf1d0a6e4207746d31a1df269f74a36b08244a1a810a7653e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
